{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "126a24a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "126a24a5",
        "outputId": "bf440c5f-3190-40d5-bd96-819941c49897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-01-25 12:17:25--  http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
            "Resolving vision.stanford.edu (vision.stanford.edu)... 171.64.68.10\n",
            "Connecting to vision.stanford.edu (vision.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 793579520 (757M) [application/x-tar]\n",
            "Saving to: ‘/root/work/data_augmentation/data/images.tar.2’\n",
            "\n",
            "images.tar.2        100%[===================>] 756.82M  16.4MB/s    in 52s     \n",
            "\n",
            "2026-01-25 12:18:18 (14.5 MB/s) - ‘/root/work/data_augmentation/data/images.tar.2’ saved [793579520/793579520]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/work/data_augmentation/data\n",
        "!wget \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\" -P ~/work/data_augmentation/data\n",
        "!tar -xf ~/work/data_augmentation/data/images.tar -C ~/work/data_augmentation/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "11966535",
      "metadata": {
        "id": "11966535"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import random\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96e9f54",
      "metadata": {
        "id": "f96e9f54"
      },
      "source": [
        "# 데이터셋 다운"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d2ea5e30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ea5e30",
        "outputId": "a91d84e5-f812-4c22-e716-6a5c60ca1f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 클래스 개수: 120\n",
            "첫 5개 클래스: ['n02085620-Chihuahua', 'n02085782-Japanese_spaniel', 'n02085936-Maltese_dog', 'n02086079-Pekinese', 'n02086240-Shih-Tzu']\n"
          ]
        }
      ],
      "source": [
        "dataset_dir = \"~/work/data_augmentation/data/Images/\"\n",
        "\n",
        "# 기본 ImageFolder (Transform은 아래 함수에서 처리)\n",
        "full_dataset = ImageFolder(root=dataset_dir)\n",
        "class_names = full_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f\"총 클래스 개수: {len(class_names)}\")\n",
        "print(f\"첫 5개 클래스: {class_names[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb11ac8",
      "metadata": {
        "id": "8fb11ac8"
      },
      "source": [
        "# 데이터셋 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fe567bc4",
      "metadata": {
        "id": "fe567bc4"
      },
      "outputs": [],
      "source": [
        "total_size = len(full_dataset)\n",
        "train_size = int(0.583 * total_size)\n",
        "test_size = total_size - train_size\n",
        "ds_train_raw, ds_test_raw = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "train_indices = set(ds_train_raw.indices)\n",
        "test_indices = set(ds_test_raw.indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19dcdbf8",
      "metadata": {
        "id": "19dcdbf8"
      },
      "source": [
        "# 데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cbafb287",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbafb287",
        "outputId": "e66dfd0a-bd71-4689-aaa5-6cca172bfeca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터 개수: 11998\n",
            "테스트 데이터 개수: 8582\n",
            "중복된 인덱스 개수: 0\n",
            "결과: 인덱스 수준에서 중복이 전혀 없습니다.\n"
          ]
        }
      ],
      "source": [
        "duplicates = train_indices.intersection(test_indices)\n",
        "\n",
        "print(f\"훈련 데이터 개수: {len(train_indices)}\")\n",
        "print(f\"테스트 데이터 개수: {len(test_indices)}\")\n",
        "print(f\"중복된 인덱스 개수: {len(duplicates)}\")\n",
        "\n",
        "if len(duplicates) == 0:\n",
        "    print(\"결과: 인덱스 수준에서 중복이 전혀 없습니다.\")\n",
        "else:\n",
        "    print(f\"경고: {len(duplicates)}개의 인덱스가 중복되었습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d108613b",
      "metadata": {
        "id": "d108613b"
      },
      "source": [
        "# 이미지 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1c844661",
      "metadata": {
        "id": "1c844661"
      },
      "outputs": [],
      "source": [
        "def show_random_samples(dataset, class_names, num_samples=5):\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    if isinstance(dataset, list):\n",
        "        images, labels = dataset[0]\n",
        "        max_idx = len(images)\n",
        "        indices = random.sample(range(max_idx), min(num_samples, max_idx))\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            img = images[idx].permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "            img = (img * 0.5) + 0.5\n",
        "            img = np.clip(img, 0, 1)\n",
        "\n",
        "            lbl_data = labels[idx]\n",
        "            lbl_idx = lbl_data.argmax().item() if lbl_data.dim() > 0 else int(lbl_data.item())\n",
        "\n",
        "            plt.subplot(1, num_samples, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Tensor: {class_names[lbl_idx].split('-')[-1]}\", fontsize=10)\n",
        "            plt.axis('off')\n",
        "\n",
        "    else:\n",
        "        max_idx = len(dataset)\n",
        "        indices = random.sample(range(max_idx), min(num_samples, max_idx))\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            img, lbl = dataset[idx]\n",
        "\n",
        "            plt.subplot(1, num_samples, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"PIL: {class_names[lbl].split('-')[-1]}\", fontsize=10)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda7cb20",
      "metadata": {
        "id": "cda7cb20"
      },
      "source": [
        "# 기본 Transform 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a8bea467",
      "metadata": {
        "id": "a8bea467"
      },
      "outputs": [],
      "source": [
        "basic_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b6c78f81",
      "metadata": {
        "id": "b6c78f81"
      },
      "outputs": [],
      "source": [
        "augment1 = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f97946c",
      "metadata": {
        "id": "8f97946c"
      },
      "source": [
        "# Mixup & CutMix 용 Transform 함수들"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a289ce73",
      "metadata": {
        "id": "a289ce73"
      },
      "outputs": [],
      "source": [
        "def onehot(label, num_classes=120):\n",
        "    if not isinstance(label, torch.Tensor):\n",
        "        label = torch.tensor(label)\n",
        "\n",
        "    return F.one_hot(label, num_classes=num_classes).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "67b49b63",
      "metadata": {
        "id": "67b49b63"
      },
      "outputs": [],
      "source": [
        "def get_clip_box(image_shape):\n",
        "    h, w = image_shape[1], image_shape[2]\n",
        "\n",
        "    x = torch.randint(0, w, (1,)).item()\n",
        "    y = torch.randint(0, h, (1,)).item()\n",
        "\n",
        "    cut_rat = torch.sqrt(1. - torch.rand(1)).item()\n",
        "    cut_w = int(w * cut_rat)\n",
        "    cut_h = int(h * cut_rat)\n",
        "\n",
        "    x_min = max(0, x - cut_w // 2)\n",
        "    y_min = max(0, y - cut_h // 2)\n",
        "    x_max = min(w, x + cut_w // 2)\n",
        "    y_max = min(h, y + cut_h // 2)\n",
        "\n",
        "    return x_min, y_min, x_max, y_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d231c817",
      "metadata": {
        "id": "d231c817"
      },
      "outputs": [],
      "source": [
        "def mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max):\n",
        "    mixed_img = image_a.clone()\n",
        "    mixed_img[:, y_min:y_max, x_min:x_max] = image_b[:, y_min:y_max, x_min:x_max]\n",
        "\n",
        "    return mixed_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d9bed2f",
      "metadata": {
        "id": "7d9bed2f"
      },
      "outputs": [],
      "source": [
        "def mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max, img_size, num_classes):\n",
        "    mixed_area = (x_max - x_min) * (y_max - y_min)\n",
        "    total_area = img_size * img_size\n",
        "    ratio = mixed_area / total_area\n",
        "\n",
        "    if label_a.ndim == 0 or label_a.size(0) != num_classes:\n",
        "        label_a = F.one_hot(label_a.to(torch.int64), num_classes=num_classes).float()\n",
        "    if label_b.ndim == 0 or label_b.size(0) != num_classes:\n",
        "        label_b = F.one_hot(label_b.to(torch.int64), num_classes=num_classes).float()\n",
        "\n",
        "    mixed_label = (1 - ratio) * label_a + ratio * label_b\n",
        "\n",
        "    return mixed_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "446b62bd",
      "metadata": {
        "id": "446b62bd"
      },
      "outputs": [],
      "source": [
        "def cutmix(images, labels, prob=1.0, img_size=224, num_classes=120):\n",
        "    current_batch_size = images.size(0)\n",
        "\n",
        "    mixed_imgs = []\n",
        "    mixed_labels = []\n",
        "\n",
        "    for i in range(current_batch_size):\n",
        "        image_a, label_a = images[i], labels[i]\n",
        "\n",
        "        if torch.rand(1).item() < prob:\n",
        "            j = torch.randint(0, current_batch_size, (1,)).item()\n",
        "            image_b, label_b = images[j], labels[j]\n",
        "\n",
        "            x_min, y_min, x_max, y_max = get_clip_box(image_a.shape)\n",
        "\n",
        "            mixed_imgs.append(mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max))\n",
        "            mixed_labels.append(mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max, img_size, num_classes))\n",
        "\n",
        "        else:\n",
        "            mixed_imgs.append(image_a)\n",
        "            mixed_labels.append(F.one_hot(label_a.to(torch.int64), num_classes=num_classes).float())\n",
        "\n",
        "    mixed_imgs = torch.stack(mixed_imgs)\n",
        "    mixed_labels = torch.stack(mixed_labels)\n",
        "\n",
        "    return mixed_imgs, mixed_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "19376b32",
      "metadata": {
        "id": "19376b32"
      },
      "outputs": [],
      "source": [
        "def mixup_2_images(image_a, image_b, label_a, label_b, num_classes=120):\n",
        "    ratio = torch.rand(1).item()\n",
        "\n",
        "    if not isinstance(label_a, torch.Tensor) or label_a.ndim == 0:\n",
        "        label_a = F.one_hot(torch.tensor(label_a).to(torch.int64), num_classes=num_classes).float()\n",
        "    if not isinstance(label_b, torch.Tensor) or label_b.ndim == 0:\n",
        "        label_b = F.one_hot(torch.tensor(label_b).to(torch.int64), num_classes=num_classes).float()\n",
        "\n",
        "    mixed_image = (1 - ratio) * image_a + ratio * image_b\n",
        "    mixed_label = (1 - ratio) * label_a + ratio * label_b\n",
        "\n",
        "    return mixed_image, mixed_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4ba4d237",
      "metadata": {
        "id": "4ba4d237"
      },
      "outputs": [],
      "source": [
        "def mixup(images, labels, img_size=224, num_classes=120):\n",
        "    current_batch_size = images.size(0)\n",
        "\n",
        "    mixed_imgs = []\n",
        "    mixed_labels = []\n",
        "\n",
        "    for i in range(current_batch_size):\n",
        "        image_a, label_a = images[i], labels[i]\n",
        "        j = torch.randint(0, current_batch_size, (1,)).item()\n",
        "        image_b, label_b = images[j], labels[j]\n",
        "\n",
        "        mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b, num_classes)\n",
        "\n",
        "        mixed_imgs.append(mixed_img)\n",
        "        mixed_labels.append(mixed_label)\n",
        "\n",
        "    mixed_imgs = torch.stack(mixed_imgs).view(current_batch_size, 3, img_size, img_size)\n",
        "    mixed_labels = torch.stack(mixed_labels).view(current_batch_size, num_classes)\n",
        "\n",
        "    return mixed_imgs, mixed_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3786155",
      "metadata": {
        "id": "d3786155"
      },
      "source": [
        "# Transform 함수들 최종 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5d36c1f6",
      "metadata": {
        "id": "5d36c1f6"
      },
      "outputs": [],
      "source": [
        "def apply_normalize_on_dataset(dataset, is_test=False, batch_size=16, with_aug=False, with_cutmix=False, with_mixup=False):\n",
        "    data_list = []\n",
        "\n",
        "    for img, lbl in dataset:\n",
        "        if not is_test and with_aug:\n",
        "            img = augment1(img)\n",
        "        img_t = basic_transform(img)\n",
        "\n",
        "        data_list.append((img_t, lbl))\n",
        "\n",
        "    dataloader = DataLoader(data_list, batch_size=batch_size, shuffle=not is_test)\n",
        "\n",
        "    processed_batches = []\n",
        "\n",
        "    for imgs, lbls in dataloader:\n",
        "        lbls_oh = onehot(lbls, num_classes=num_classes)\n",
        "\n",
        "        if not is_test and with_cutmix:\n",
        "            imgs, lbls_oh = cutmix(imgs, lbls_oh)\n",
        "        elif not is_test and with_mixup:\n",
        "            imgs, lbls_oh = mixup(imgs, lbls_oh)\n",
        "\n",
        "        processed_batches.append((imgs, lbls_oh))\n",
        "\n",
        "    return processed_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0688fa",
      "metadata": {
        "id": "1d0688fa"
      },
      "outputs": [],
      "source": [
        "ds_test = apply_normalize_on_dataset(ds_test_raw, is_test=True)\n",
        "ds_no_aug = apply_normalize_on_dataset(ds_train_raw, is_test=False, with_aug=False)\n",
        "ds_aug = apply_normalize_on_dataset(ds_train_raw, is_test=False, with_aug=True)\n",
        "ds_mixup = apply_normalize_on_dataset(ds_train_raw, is_test=False, with_aug=True, with_mixup=True)\n",
        "ds_cutmix = apply_normalize_on_dataset(ds_train_raw, is_test=False, with_aug=True, with_cutmix=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46d059f",
      "metadata": {
        "id": "b46d059f"
      },
      "source": [
        "# 변환 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d02df2",
      "metadata": {
        "id": "35d02df2"
      },
      "outputs": [],
      "source": [
        "show_random_samples(ds_train_raw, class_names)\n",
        "show_random_samples(ds_aug, class_names)\n",
        "show_random_samples(ds_mixup, class_names)\n",
        "show_random_samples(ds_cutmix, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b992443",
      "metadata": {
        "id": "8b992443"
      },
      "source": [
        "# 학습 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f3ce7dc",
      "metadata": {
        "id": "0f3ce7dc"
      },
      "outputs": [],
      "source": [
        "def implementation(model, train_loader, test_loader, epochs=3):\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_acc': [], 'val_acc': []}\n",
        "    train_loss_list = []\n",
        "    train_accuracy_list = []\n",
        "    test_loss_list = []\n",
        "    test_accuracy_list = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            _, targets = labels.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        train_loss = train_loss / train_size\n",
        "        train_acc = 100.0 * train_correct / train_total\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item() * images.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                _, targets = labels.max(1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_loss = test_loss / test_size\n",
        "        test_acc = 100. * test_correct / test_total\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        test_loss_list.append(test_loss)\n",
        "        train_accuracy_list.append(train_acc)\n",
        "        test_accuracy_list.append(test_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:>2d} - Train Loss: {train_loss:.3f}, Test Loss: {test_loss:.3f}, Train Acc: {train_acc:.3f}%, Val Acc: {test_acc:.3f}%\")\n",
        "\n",
        "    return (train_loss_list, test_loss_list, train_accuracy_list, test_accuracy_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4697e759",
      "metadata": {
        "id": "4697e759"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e09e8d2",
      "metadata": {
        "id": "6e09e8d2"
      },
      "source": [
        "# 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f454c308",
      "metadata": {
        "id": "f454c308"
      },
      "outputs": [],
      "source": [
        "# No Augmentation\n",
        "model_no_aug = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model_no_aug.fc = nn.Linear(model_no_aug.fc.in_features, num_classes)\n",
        "result_no_aug = implementation(model_no_aug, ds_no_aug, ds_test, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acca9ed0",
      "metadata": {
        "id": "acca9ed0"
      },
      "outputs": [],
      "source": [
        "# Basic Augmentation\n",
        "model_aug = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model_aug.fc = nn.Linear(model_aug.fc.in_features, num_classes)\n",
        "result_basic_aug = implementation(model_aug, ds_aug, ds_test, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79cb1f3",
      "metadata": {
        "id": "b79cb1f3"
      },
      "outputs": [],
      "source": [
        "# Basic Augmentation + MixUp\n",
        "model_mixup = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model_mixup.fc = nn.Linear(model_mixup.fc.in_features, num_classes)\n",
        "result_mixup = implementation(model_mixup, ds_mixup, ds_test, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee420eba",
      "metadata": {
        "id": "ee420eba"
      },
      "outputs": [],
      "source": [
        "# Basic Augmentation + CutMix\n",
        "model_cutmix = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model_cutmix.fc = nn.Linear(model_cutmix.fc.in_features, num_classes)\n",
        "result_cutmix = implementation(model_cutmix, ds_cutmix, ds_test, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1294fc30",
      "metadata": {
        "id": "1294fc30"
      },
      "source": [
        "# 결과 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17d5c52",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(result_no_aug[0], '-o', label='No Augment')\n",
        "plt.plot(result_basic_aug[0], '-o', label='Basic Augment')\n",
        "plt.plot(result_mixup[0], '-o', label='Mixup')\n",
        "plt.plot(result_cutmix[0], '-o', label='CutMix')\n",
        "plt.title('Train Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72036f00",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(result_no_aug[1], '-o', label='No Augment')\n",
        "plt.plot(result_basic_aug[1], '-o', label='Basic Augment')\n",
        "plt.plot(result_mixup[1], '-o', label='Mixup')\n",
        "plt.plot(result_cutmix[1], '-o', label='CutMix')\n",
        "plt.title('Train Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b63862",
      "metadata": {
        "id": "66b63862"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(result_no_aug[2], '-o', label='No Augment')\n",
        "plt.plot(result_basic_aug[2], '-o', label='Basic Augment')\n",
        "plt.plot(result_mixup[2], '-o', label='Mixup')\n",
        "plt.plot(result_cutmix[2], '-o', label='CutMix')\n",
        "plt.title('Train Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11d2cbd",
      "metadata": {
        "id": "e11d2cbd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(result_no_aug[3], '-o', label='No Augment')\n",
        "plt.plot(result_basic_aug[3], '-o', label='Basic Augment')\n",
        "plt.plot(result_mixup[3], '-o', label='Mixup')\n",
        "plt.plot(result_cutmix[3],'-o', label='CutMix')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
